# Brax Training Viewer for Real-Time Policy Visualization

This project is developed during the [Google Summer of Code 2025](https://summerofcode.withgoogle.com/programs/2025/projects/Xm0toJHl).

Reinforcement Learning (RL) training in Brax operates at high performance, utilizing JAX for efficient parallelized training. However, due to the nature of JAXâ€™s computation model, training occurs in a highly abstracted and batched manner, making it challenging to inspect agent behavior in real-time. Currently, users must either wait until training finishes to evaluate policies or extract rollout data manually, which is inefficient and restricts debugging capabilities. A Brax Training Viewer would empower users to visualize the evolution of the policy while training is ongoing, bridging the gap between RL research and practical interpretability.

The Brax Training Viewer offers an interactive, real-time visualization tool for monitoring reinforcement learning (RL) policies during training, utilizing the official PPO training function from Brax. This package allows users to synchronize the training with a MuJoCo-based viewer, showcasing the effect of the actions taken by the RL policy across numerous parallel environments. Users can benefit from real-time action updates, which enable them to track the evolution of the policy over time. A toggle option to enable or disable synchronization will be provided to enhance performance, allowing users to pause visualization and speed up the training process temporarily.

As this tool extends the official Brax PPO training function, it ensures seamless compatibility with current reinforcement learning pipelines. This allows for easy implementation without changing existing RL workflows, making it an excellent choice for researchers and practitioners needing to visualize, debug, and analyze RL policies in Brax with minimal configuration.

## Outcomes

* **Synchronized MuJoCo Visualization with Brax Training**: Extract actions generated by the policy network at each training step and apply them to a parallel running MuJoCo simulation.
* **Support Parallel Environments**: The viewer will visualize multiple parallel agents, reflecting the exact training conditions in Brax.
* **Allow Enable/Disable Rendering**: Since the copy process from GPU to CPU slows down the training process, users can dynamically enable or disable the data transfer to have high-speed training without visual feedback or keep visualization synchronized at the cost of slower training.

## Installation and Setup

### Conda
-   (Optionally) install Python virtual environment [conda](https://www.anaconda.com/docs/getting-started/miniconda/main)
-   (Optionally) create a virtual environment `conda create -n test python=3.10`
-   `conda activate test`
-   `cd` to the root folder of this repo
-   run `git submodule update --init --recursive`
-   run `pip install -e braxviewer/brax`
-   run `pip install .`
-   run `pip install -r requirements.txt `
-   (Optionally) install Jax with hardware acceleration: `pip install -U "jax[cuda12]"` or `pip install -U "jax[cuda11]"` or `pip install -U "jax[tpu]"`
-   You can try the examples in the `examples/` folder, for example: `python examples/cartpole/cartpole.py`
-   Open a web browser and go to `http://127.0.0.1:8081/` for the viewer.

### UV
[UV](https://github.com/astral-sh/uv) is an extremely fast Python package installer and resolver, written in Rust.
-   `cd` to the root folder of this repo
-   run `git submodule update --init --recursive`
-   Install uv `pip install uv`
-   Create a virtual environment `uv venv`
-   Activate the virtual environment `source .venv/bin/activate`
-   Install dependencies `uv pip install -r requirements.txt`
-   Install the project with `uv pip install -e braxviewer/brax` and `uv pip install .`
-   You can try the examples provided in the `examples/` folder, for instance: `python examples/cartpole/cartpole.py`
-   Open a web browser and navigate to `http://127.0.0.1:8081/` to see the viewer.

## How to Use

Integrating the viewer into your existing Brax training script is straightforward. The core steps are:
1. Import the necessary components.
2. Instantiate the appropriate viewer (`WebViewer` for a single instance, `WebViewerBatched` for multiple).
3. Wrap your environment instance with `ViewerWrapper`.
4. Pass the wrapped environment to the `ppo.train` function.

Below is a guide that covers both single and batched visualization.

### 1. Import Modules
First, import all the necessary modules. This set of imports works for both single and batched setups.

```python
import time
from braxviewer.WebViewer import WebViewer
from braxviewer.WebViewerBatched import WebViewerBatched
from braxviewer.brax.brax.envs.wrappers.viewer import ViewerWrapper
from braxviewer.brax.brax.training.agents.ppo import train as ppo
from brax.training.agents.ppo import networks as ppo_networks
# from your_project import YourBraxEnvironment, custom_wrap_env
```

### 2. Instantiate Viewer and Wrap Environment

First, create your environment. This step is the same for both single and batched visualization.
```python
# Your robot XML. Examples can be found in the Brax assets folder
# https://github.com/google/brax/tree/300b1079363894733fa1090c6bb055b881eb0ac1/brax/envs/assets
xml_model = "..."
env = YourBraxEnvironment(xml_model=xml_model)
```

Next, choose and instantiate the viewer based on your needs.

**Option A: For a single environment**
```python
num_parallel_envs = 1
viewer = WebViewer(xml=xml_model)
```

**Option B: For batched (parallel) environments**
```python
num_parallel_envs = 8
viewer = WebViewerBatched(num_envs=num_parallel_envs, xml=xml_model)
```

Finally, run the viewer and wrap the environment. 
```python
viewer.run()
env_for_visualization = ViewerWrapper(env=env, viewer=viewer)
```

### 3. Call the Training Function

Finally, call `ppo.train` with the wrapped environment. You may visualize either train_env or evaluate_env.

```python
# Define the PPO network
make_networks_factory = ppo_networks.make_ppo_networks

# Call the training function
make_policy_fn, params, _ = ppo.train(
    environment=env_for_visualization,
    network_factory=make_networks_factory,
    num_envs=num_parallel_envs,
    wrap_env_fn=custom_wrap_env,
)
```
